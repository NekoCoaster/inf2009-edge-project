# inf2009-edge-project

***AI Declaration: This README.md has been generated by ChatGPT***

# Setting up WSL2 with Ubuntu 22.04 and Installing CUDA & LLaVA-NeXT

This guide will help you set up a WSL2 environment on Windows with Ubuntu 22.04, install CUDA 12.1 for WSL, set up Miniconda, and clone and install the [LLaVA-NeXT](https://github.com/LLaVA-VL/LLaVA-NeXT) repository.

> **Important:**  
> You must have an **NVIDIA GPU** with drivers that **support at least CUDA 12.1**.  
> If your drivers are newer than CUDA 12.1, **you do NOT need to downgrade** your Windows GPU drivers. CUDA installations inside WSL2 are backward compatible with newer driver versions.

---

## Prerequisites

- Windows 10/11 with WSL2 enabled.
- Ubuntu 22.04 installed through WSL2.
- An NVIDIA GPU with WSL2 GPU support (and drivers supporting CUDA 12.1 or newer).
- Basic familiarity with the Linux terminal.

---

## Step 1: Install WSL2 and Ubuntu 22.04

1. **Install WSL2** by running the following command in PowerShell (Admin):

```powershell
wsl --install
```

2. **Set WSL2 as the default version** (if not already set):

```powershell
wsl --set-default-version 2
```

3. **Install Ubuntu 22.04** from the Microsoft Store.

4. **Update Ubuntu** immediately after installation:

```bash
sudo apt update && sudo apt upgrade -y
```

---

## Step 2: Verify NVIDIA GPU Support in WSL2

Check if your NVIDIA GPU is detected inside WSL:

```bash
nvidia-smi
```

You should see your GPU listed. If not, ensure you have the latest [NVIDIA drivers for WSL](https://developer.nvidia.com/cuda/wsl) installed on your Windows system.

---

## Step 3: Running the Setup Scripts

Clone this repository or copy the `phase1.sh` and `phase2.sh` scripts to your Ubuntu environment.

### 3.1. Run `phase1.sh`

This script will:

- Update the system
- Install CUDA 12.1
- Install Miniconda
- Initialize Conda for your shell

Make it executable and run:

```bash
chmod +x phase1.sh
./phase1.sh
```

**Notes:**
- After running, **open a new terminal** or **source your `.bashrc`** to refresh environment variables:

```bash
source ~/.bashrc
```

---

### 3.2. Run `phase2.sh`

This script will:

- Clone the LLaVA-NeXT repository
- Create and activate a new Conda environment (`llava_next`)
- Install LLaVA-NeXT and FlashAttention

Make it executable and run:

```bash
chmod +x phase2.sh
./phase2.sh
```

If you encounter any **CRC-32 errors** during `pip install`, simply **re-run** the same `pip install` command:

```bash
pip install -e ".[train]"
```

---

## Notes on CUDA Driver Version

- The installed CUDA 12.1 inside WSL2 **does not need to exactly match** your Windows host driver version.
- **Higher driver versions are backward compatible** with CUDA 12.1.  
- **Do not downgrade** your Windows drivers unless specifically necessary.

---

## Useful Commands

- **Activate Conda environment manually** (if needed):

```bash
conda activate llava_next
```

- **Update Conda**:

```bash
conda update conda
```

- **Check GPU availability in Python**:

```python
import torch
print(torch.cuda.is_available())
print(torch.cuda.get_device_name(0))
```

---

## Troubleshooting

- If `nvidia-smi` doesn’t work inside WSL2:
- Make sure your Windows NVIDIA drivers are up to date.
- Ensure WSL2 integration is enabled in NVIDIA Control Panel.

- If Conda isn't found after `phase1.sh`:
- Try opening a new terminal or run `source ~/.bashrc`.

## Step 4: Running the Model Testing Scripts

After successfully setting up LLaVA-NeXT, you can run the following **testing scripts** to verify the model installation and test its integration with MQTT.

These scripts test different behaviors of the AI model and MQTT setup.

---

### 4.1 Purpose of Each Testing Script

| Script Name | Purpose |
|:------------|:--------|
| `test.py` | Quick standalone sanity check. Downloads a sample image and asks the model a simple Yes/No question ("Is there a Coca-Cola can in this image?"). |
| `server.py` | Minimal object detection server. Listens for camera images over MQTT, detects the target object, and publishes the detection result ("YES" or "NO"). |
| `server2.py` | Same as `server.py` but follows a slightly different MQTT topic structure (e.g., command topic is `team24/fog/goal`). |
| `server2_1.py` | Same as `server2.py` but handles minor topic naming differences (e.g., `team24/command/objective`). |
| `server4.py` | Full object detection **plus** navigation decision server. Listens to both **camera** and **navigation camera** images, detects objects, and outputs driving directions like W, A, S, D, or STOP. |

---

### 4.2 Installing Additional Python Dependencies

Make sure you have these installed inside your **`llava_next`** conda environment:

```bash
conda activate llava_next
pip install paho-mqtt pillow requests
```

---

### 4.3 Running Each Script

> **Important:**  
> Before running the server scripts, **edit the MQTT settings** inside each `.py` file:
>
> - `MQTT_BROKER`
> - `MQTT_PORT`
> - `MQTT_USERNAME`
> - `MQTT_PASSWORD`

Otherwise, the scripts will not be able to connect to your MQTT broker.

---

#### To run the standalone model test (`test.py`):

```bash
python test.py
```

You should see the model answer a Yes/No question based on a sample image.

---

#### To run the minimal object detection server (`server.py`, `server2.py`, or `server2_1.py`):

```bash
python server.py
```

or

```bash
python server2.py
```

or

```bash
python server2_1.py
```

The server will:

- Listen for incoming **camera** images.
- Answer whether the specified object is present.
- Publish "YES" or "NO" results over MQTT.

---

#### To run the full object detection + navigation server (`server4.py`):

```bash
python server4.py
```

The server will:

- Listen for both **camera** and **navigation camera** images.
- Detect the object (YES/NO).
- Make a driving decision (W/A/S/D/STOP) based on navigation images.

> **Tip:** `server4.py` is the most complete version for full ROV operation (object detection + navigation decision).

---

### 4.4 Stopping the Servers

All servers can be stopped safely with `Ctrl + C`.  
They will cleanly disconnect from the MQTT broker before exiting.

---

### Recommended First-Time Test

1. Run `test.py` first to verify the model loads correctly and answers simple questions.
2. Then run `server.py` or `server2.py` to check MQTT-based object detection.
3. Finally, run `server4.py` to test full ROV behavior (object detection + navigation).

---

## Step 5: Running the MQTT Image Viewer Scripts

There are two scripts available for live MQTT-based image preview:

- `mqtt_image_preview.py`
- `mqtt_image_preview_navcam.py`

Both scripts are almost identical, designed to connect to an MQTT broker, receive Base64-encoded image data, and display the images in real-time using a simple Tkinter GUI.

---

### 5.1 Install Required Dependencies

First, make sure you have the necessary Python packages installed:

```bash
conda activate llava_next
pip install paho-mqtt pillow
```

> **Note:**  
> These scripts use **Tkinter** for the GUI, which should already be included with the default Python installation on Ubuntu.  
> If not, you can install it manually:

```bash
sudo apt install python3-tk
```

---

### 5.2 Configure the Scripts

Before running either script, **edit the following fields** inside the `.py` files:

- `MQTT_BROKER` → your broker's domain or IP address.
- `MQTT_PORT` → default is `1883` (change only if your broker uses a different port).
- `MQTT_TOPIC` → the topic where images are published (default: `team24/rov/camera`).
- `MQTT_USERNAME` → your MQTT username.
- `MQTT_PASSWORD` → your MQTT password.

Example:

```python
MQTT_BROKER = "broker.example.com"
MQTT_PORT = 1883
MQTT_TOPIC = "team24/rov/camera"
MQTT_USERNAME = "yourUsername"
MQTT_PASSWORD = "yourPassword"
```

---

### 5.3 Running the Scripts

Once dependencies are installed and the script is configured, simply run either script using:

```bash
python mqtt_image_preview.py
```

or

```bash
python mqtt_image_preview_navcam.py
```

A GUI window will appear that automatically updates whenever a new image is received over MQTT.

---

## Step 6: Optional Tips

- If you encounter `Tkinter` or `paho-mqtt` errors, double-check that you are inside the **`llava_next`** conda environment.
- You can have both scripts running at the same time if you are listening to **different topics** (e.g., `camera_front`, `camera_navcam`).
- Use `conda deactivate` to exit the environment after you are done.
